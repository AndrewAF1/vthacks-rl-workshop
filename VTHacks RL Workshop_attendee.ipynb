{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VTHacks 8: Deep Reinforcement Learning\n",
    "### GitHub Repo: https://github.com/AndrewAF1/vthacks-rl-workshop\n",
    "### Presentors: Andrew Farabow (aafarabow@vt.edu), Patrick Riley (rileyp@vt.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is reinforcement learning?\n",
    "\n",
    "### Short Answer\n",
    "Using intelligent trial and error to solve problems\n",
    "\n",
    "### Long Answer \n",
    "A reinforcement learning problem consists of an agent which interacts with an environment in discrete timesteps. At each timestep the agent is given an observation $s$, takes an action a based on policy $\\pi$, and receives a reward $r$. The policy, which defines an agentâ€™s behavior, is a function that maps the state to a probability distribution over the (discrete or continuous) set of possible actions. The defining characteristic of deep reinforcement learning is that the policy is represented by a neural network, which is generally trained via some variant of stochastic gradient descent to maximize expected reward. In order to make a challenge learnable via RL, one must define an observation and action space. These are vectors that define the shape and range of possible values that can compose the observation and action. The observation space consists of the features of the game we let the agent learn to make decisions from, and the action space is determined by what parts of the environment we decide to have the agent control. \n",
    "\n",
    "### Translation\n",
    "RL involves giving an \"agent\" a goal (along with a way of measuring that goal) and allowing to figure out how to reach it on its own. It does this by playing the game repeatedly and trying to improve its score by combining exploration (trying new things) with exploitation (doing what it has learned works well).\n",
    "\n",
    "\n",
    "## What does the \"deep\" part mean?\n",
    "There are many ways to solve RL problems. \"Deep\" means that a neural network is used to represent the \"brain\" of the algorithm - it encodes the method by which the agent makes decisions and allows that method to be updated as new data becomes available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we are going to do today\n",
    "* solve a classic reinforcement learning problem, Cartpole\n",
    "* explore a basic RL algorithm, Deep Q-Learning\n",
    "* tune various parameters and see how that affects learning\n",
    "\n",
    "## Our tools\n",
    "* Python (NOTE: ADD LINKS LATER)\n",
    "* PyTorch\n",
    "* OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cartpole](https://i.redd.it/sqjzj2cgnpt21.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets start the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we are going to set up a few parameters that will be used in training. Some won't make sense yet.\n",
    "They are named after the Greek letters used in the equations from the DQN paper. The person who wrote this code is cruel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "\n",
    "env = gym.make('CartPole-v0') # creates an OpenAI Gym environment, an object you interact with to step through the game\n",
    "epsilon = .01                 # controls the exploration vs exploitation balance\n",
    "gamma = .99                   # reward discount factor\n",
    "tau = .995                    # controls how quickly we update the target network\n",
    "random.seed(666)              # seed the random number generator for reproducibility\n",
    "batch_size = 128              # how much to sample from the replay buffer at a time\n",
    "max_ep = 500                  # number of games to play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a neural network in PyTorch. The `nn.Linear...` lines represent fully connected layers and the `nn.ReLU..` lines are activation functions, which allow the network to represent non-linear functions. The `forward` function gets called when you want to get the output of the model for some data (called a forward pass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Q(nn.Module):\n",
    "    def __init__(self,env):\n",
    "        super(Q, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.shape[0], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, env.action_space.n)\n",
    "        )\n",
    "\n",
    "    def forward(self, s):\n",
    "        return self.main(torch.FloatTensor(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the replay buffer, which stores the states, actions, rewards, next state, and masks (more on those later) for every frame of the game. We will use this data to update the neural network later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=int(size))\n",
    "        self.maxSize = size\n",
    "        self.len = 0\n",
    "\n",
    "    def sample(self, count):\n",
    "        count = min(count, self.len)\n",
    "        batch = random.sample(self.buffer, count)\n",
    "\n",
    "        s_arr = torch.FloatTensor(np.array([arr[0] for arr in batch]))\n",
    "        a_arr = torch.FloatTensor(np.array([arr[1] for arr in batch]))\n",
    "        r_arr = torch.FloatTensor(np.array([arr[2] for arr in batch]))\n",
    "        s2_arr = torch.FloatTensor(np.array([arr[3] for arr in batch]))\n",
    "        m_arr = torch.FloatTensor(np.array([arr[4] for arr in batch]))\n",
    "\n",
    "        return s_arr, a_arr.unsqueeze(1), r_arr.unsqueeze(1), s2_arr, m_arr.unsqueeze(1)\n",
    "\n",
    "    def len(self):\n",
    "        return self.len\n",
    "\n",
    "    def store(self, s, a, r, s2, d):\n",
    "        def fix(x):\n",
    "            if not isinstance(x, np.ndarray): return np.array(x)\n",
    "            else: return x\n",
    "\n",
    "        data = [s, np.array(a,dtype=np.float64), r, s2, 1 - d]\n",
    "        transition = tuple(fix(x) for x in data)\n",
    "        self.len = min(self.len + 1, self.maxSize)\n",
    "        self.buffer.append(transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function explores the environment for the specified number of timesteps to improve the performance of the DQN. Gets run at the begining of training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore(timestep):\n",
    "    ts = 0\n",
    "    while ts < timestep:\n",
    "        s = env.reset()\n",
    "        while True:\n",
    "            a = env.action_space.sample()\n",
    "            s2, r, done, _ = env.step(int(a))\n",
    "            rb.store(s, a, r, s2, done)\n",
    "            ts += 1\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                s = s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code updates the Q by sampling past states from the buffer, comparing the network's estimate of a state's potential future rewards versus the actual future rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update():\n",
    "    s, a, r, s2, m = rb.sample(batch_size) # get a batch of states, actions, reward, next states and\n",
    "                                           # masks (1 for game over, else zero) from the replay buffer\n",
    "\n",
    "    with torch.no_grad(): # don't track gradients when you pass through the target network\n",
    "        max_next_q, _ = q_target(s2).max(dim=1, keepdim=True) # get the next state value from the target net\n",
    "        \n",
    "        # sum rewards with discount penalty to avoid infinite time horizons\n",
    "        # reward + \"doneness\" * discount factor * anticipated future reward as predicted by (target) Q network\n",
    "        #y = ## FILL IN CODE HERE\n",
    "    \n",
    "    q_estimates = torch.gather(q(s), 1, a.long())\n",
    "    \n",
    "    # calulcate the MSE loss between the q_estimates and y\n",
    "    #loss = ## FILL IN CODE HERE\n",
    "\n",
    "    # update Q network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # update target Q networked with weighted avereging\n",
    "    for param, target_param in zip(q.parameters(), q_target.parameters()):\n",
    "        target_param.data = target_param.data*tau + param.data*(1-tau)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main body of our algorithm, where we use all the bits we defined above to step through the enviornment and learn from experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the Q network and target network, a copy that lags behind to smooth training\n",
    "q = Q(env)\n",
    "q_target = deepcopy(q)\n",
    "\n",
    "# init the optimizer, which uses the loss to update the weights of the network\n",
    "optimizer = torch.optim.Adam(q.parameters(), lr=1e-3)\n",
    "\n",
    "# init the replay buffer and do some exploration to fill it\n",
    "rb = ReplayBuffer(1e6)\n",
    "explore(10000) \n",
    "\n",
    "# training loop\n",
    "ep = 0\n",
    "while ep < max_ep: # loop through some number of episodes, aka complete games\n",
    "    s = env.reset() # reset the environment at the state of the game\n",
    "    ep_r = 0\n",
    "    while True: # loop through a game frame by frame\n",
    "        with torch.no_grad():\n",
    "            # epsilon greedy exploration\n",
    "            if random.random() < epsilon: # some portion of the time\n",
    "                # pick a random action to aid in exploration\n",
    "                #a = # FILL IN CODE HERE\n",
    "                pass\n",
    "            else: # the rest of the time\n",
    "                # get the action recomended by the network\n",
    "                #a = # FILL IN CODE HERE\n",
    "                pass \n",
    "        \n",
    "        # take a step in the environment and get the resulting next state, reward, and done boolean\n",
    "        # FILL IN CODE HERE\n",
    "        \n",
    "        rb.store(s, a, r, s2, done) # store in the replay buffer\n",
    "        ep_r += r # update episode reward\n",
    "\n",
    "        \n",
    "        if done: # if a ga,e endsbreak the loop and begin again, \n",
    "            if ep % 10 == 0:\n",
    "                print(f\"Episode {ep} Reward: {ep_r}\")\n",
    "            ep += 1\n",
    "            break\n",
    "        else: # otherwise continue\n",
    "            s = s2\n",
    "\n",
    "        update() # compute loss and update the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congrats, thats deep Q learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps for the curious\n",
    "* read the research paper and dig into the math: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "* check out other algorithms like Policy Gradients and see how they compare\n",
    "* read more about PyTroch\n",
    "* read my [blog posts](https://computable.ai/category/fundamentals.html) (shameless-self promotion) \n",
    "* read Sutton and Barto's [Reinforcement Learning, second edition: An Introduction](https://www.amazon.com/Reinforcement-Learning-Introduction-Adaptive-Computation/dp/0262039249/ref=pd_all_pref_1/146-5524833-7295228?_encoding=UTF8&pd_rd_i=0262039249&pd_rd_r=9ab2599f-5305-49ee-b05e-6c09ab9771d6&pd_rd_w=StfTr&pd_rd_wg=Oa0y3&pf_rd_p=e6474b7e-8fb6-4ee2-b5d6-a1da55185fe6&pf_rd_r=CRDR50SPECKZ3PCCJ2NQ&psc=1&refRID=CRDR50SPECKZ3PCCJ2NQ) (for the mathematically inclined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('mlenv': conda)",
   "language": "python",
   "name": "python37464bitmlenvconda1b9674ec19d24c2d994779ac3cd6fdf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
