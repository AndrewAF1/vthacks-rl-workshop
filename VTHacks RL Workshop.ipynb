{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VTHacks 8: Deep Reinforcement Learning\n",
    "### GitHub Repo: https://github.com/AndrewAF1/vthacks-rl-workshop\n",
    "### Presentors: Andrew Farabow (aafarabow@vt.edu), Patrick Riley (rileyp@vt.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is reinforcement learning?\n",
    "\n",
    "### Short Answer\n",
    "Using intelligent trial and error to solve problems\n",
    "\n",
    "### Long Answer \n",
    "A reinforcement learning problem consists of an agent which interacts with an environment in discrete timesteps. At each timestep the agent is given an observation $s$, takes an action a based on policy $\\pi$, and receives a reward $r$. The policy, which defines an agentâ€™s behavior, is a function that maps the state to a probability distribution over the (discrete or continuous) set of possible actions. The defining characteristic of deep reinforcement learning is that the policy is represented by a neural network, which is generally trained via some variant of stochastic gradient descent to maximize expected reward. In order to make a challenge learnable via RL, one must define an observation and action space. These are vectors that define the shape and range of possible values that can compose the observation and action. The observation space consists of the features of the game we let the agent learn to make decisions from, and the action space is determined by what parts of the environment we decide to have the agent control. \n",
    "\n",
    "### Translation\n",
    "RL involves giving an \"agent\" a goal (along with a way of measuring that goal) and allowing to figure out how to reach it on its own. It does this by playing the game repeatedly and trying to improve its score by combining exploration (trying new things) with exploitation (doing what it has learned works well).\n",
    "\n",
    "\n",
    "## What does the \"deep\" part mean?\n",
    "There are many ways to solve RL problems. \"Deep\" means that a neural network is used to represent the \"brain\" of the algorithm - it encodes the method by which the agent makes decisions and allows that method to be updated as new data becomes available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What we are gonna get done today\n",
    "* solve a classic reinforcement learning problem, Cartpole\n",
    "* explore a basic RL algorithm, Deep Q-Learning\n",
    "* tune various parameters and see how that affects learning\n",
    "\n",
    "## Tools we are using to get there\n",
    "* Python (NOTE: ADD LINKS LATER)\n",
    "* PyTorch\n",
    "* OpenAI Gym\n",
    "* Visdom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cartpole](https://i.redd.it/sqjzj2cgnpt21.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets start the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to the cool stuff, lets give ourselves the ability to visualize our model's improvement with a tool called Visdom, from Facebook Research. If you are following along, you need to open a terminal and run visdom (NOTE: ADD INSTRUCTIONS FOR WINDOWS AND PIPENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from visdom import Visdom\n",
    "\n",
    "viz = Visdom()\n",
    "\n",
    "win = None\n",
    "\n",
    "def update_viz(ep, ep_reward, algo):\n",
    "    global win\n",
    "\n",
    "    if win is None:\n",
    "        win = viz.line(\n",
    "            X=np.array([ep]),\n",
    "            Y=np.array([ep_reward]),\n",
    "            win=algo,\n",
    "            opts=dict(\n",
    "                title=algo,\n",
    "                fillarea=False\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        viz.line(\n",
    "            X=np.array([ep]),\n",
    "            Y=np.array([ep_reward]),\n",
    "            win=win,\n",
    "            update='append',\n",
    "            xaxis='Episodes',\n",
    "            yaxis='Reward'\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below creates a neural network in PyTorch. The `nn.Linear...` lines represent fully connected layers and the `nn.ReLU..` lines are activation functions, which allow the network to represent non-linear functions. The `forward` function gets called when you want to get the output of the model for some data (called a forward pass)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Q(nn.Module):\n",
    "    def __init__(self,env):\n",
    "        super(Q, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.shape[0], 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, env.action_space.n)\n",
    "        )\n",
    "\n",
    "    def forward(self, s):\n",
    "        return self.main(torch.FloatTensor(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: EXPLAIN STUFF HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo_name = 'DQN'\n",
    "env = gym.make('CartPole-v0')\n",
    "epsilon = .01\n",
    "gamma = .99\n",
    "#Proportion of network you want to keep\n",
    "tau = .995\n",
    "random.seed(666)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: REMOVE TARGET NETWORK AND EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updates the Q by taking the max action and then calculating the loss based on a target\n",
    "def update():\n",
    "    s, a, r, s2, m = rb.sample(batch_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        max_next_q, _ = q_target(s2).max(dim=1, keepdim=True)\n",
    "        y = r + m*gamma*max_next_q\n",
    "    loss = F.mse_loss(torch.gather(q(s), 1, a.long()), y)\n",
    "\n",
    "    #Update q\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    #Update q_target\n",
    "    for param, target_param in zip(q.parameters(), q_target.parameters()):\n",
    "        target_param.data = target_param.data*tau + param.data*(1-tau)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explores the environment for the specified number of timesteps to improve the performance of the DQN\n",
    "def explore(timestep):\n",
    "    ts = 0\n",
    "    while ts < timestep:\n",
    "        s = env.reset()\n",
    "        while True:\n",
    "            a = env.action_space.sample()\n",
    "            s2, r, done, _ = env.step(int(a))\n",
    "            rb.store(s, a, r, s2, done)\n",
    "            ts += 1\n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                s = s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=int(size))\n",
    "        self.maxSize = size\n",
    "        self.len = 0\n",
    "\n",
    "    def sample(self, count):\n",
    "        count = min(count, self.len)\n",
    "        batch = random.sample(self.buffer, count)\n",
    "\n",
    "        s_arr = torch.FloatTensor(np.array([arr[0] for arr in batch]))\n",
    "        a_arr = torch.FloatTensor(np.array([arr[1] for arr in batch]))\n",
    "        r_arr = torch.FloatTensor(np.array([arr[2] for arr in batch]))\n",
    "        s2_arr = torch.FloatTensor(np.array([arr[3] for arr in batch]))\n",
    "        m_arr = torch.FloatTensor(np.array([arr[4] for arr in batch]))\n",
    "\n",
    "        return s_arr, a_arr.unsqueeze(1), r_arr.unsqueeze(1), s2_arr, m_arr.unsqueeze(1)\n",
    "\n",
    "    def len(self):\n",
    "        return self.len\n",
    "\n",
    "    def store(self, s, a, r, s2, d):\n",
    "        def fix(x):\n",
    "            if not isinstance(x, np.ndarray): return np.array(x)\n",
    "            else: return x\n",
    "\n",
    "        data = [s, np.array(a,dtype=np.float64), r, s2, 1 - d]\n",
    "        transition = tuple(fix(x) for x in data)\n",
    "        self.len = min(self.len + 1, self.maxSize)\n",
    "        self.buffer.append(transition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: BREAK INTO MANY CELLS, ADD RENDER TO LOOP, AND EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = Q(env)\n",
    "q_target = deepcopy(q)\n",
    "\n",
    "optimizer = torch.optim.Adam(q.parameters(), lr=1e-3)\n",
    "max_ep = 1000\n",
    "\n",
    "batch_size = 128\n",
    "rb = ReplayBuffer(1e6)\n",
    "\n",
    "explore(10000)\n",
    "ep = 0\n",
    "while ep < max_ep:\n",
    "    s = env.reset()\n",
    "    ep_r = 0\n",
    "    while True:\n",
    "        with torch.no_grad():\n",
    "            #Epsilon greed exploration\n",
    "            if random.random() < epsilon:\n",
    "                a = env.action_space.sample()\n",
    "            else:\n",
    "                a = int(np.argmax(q(s)))\n",
    "        #Get the next state, reward, and info based on the chosen action\n",
    "        s2, r, done, _ = env.step(int(a))\n",
    "        rb.store(s, a, r, s2, done)\n",
    "        ep_r += r\n",
    "\n",
    "        #If it reaches a terminal state then break the loop and begin again, otherwise continue\n",
    "        if done:\n",
    "            update_viz(ep, ep_r, algo_name)\n",
    "            ep += 1\n",
    "            break\n",
    "        else:\n",
    "            s = s2\n",
    "\n",
    "        update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: CONCLUSION HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('mlenv': conda)",
   "language": "python",
   "name": "python37464bitmlenvconda1b9674ec19d24c2d994779ac3cd6fdf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
